{"title":"Bayesian Analysis of Linear Regression Model","markdown":{"headingText":"Bayesian Analysis of Linear Regression Model","containsRefs":false,"markdown":"\n\\section*{Posterior derivations for a simple linear regression model}\n\n> This document presents the derivations of the posterior distributions for a simple linear regression model. It considers cases of a natural-conjugate and conditionally-conjugate prior distributions and presents the analytical derivations of the joint posterior distribution for the former, the Gibbs sampler for the latter case.\n\n\n\n## The model\n\nConsider the following linear regression model:\n\\begin{align} \nY &= \\beta X + E\\\\\nE|X &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2I_T\\right),\n\\end{align} \nwhere $Y$ and $X$ are $T\\times1$ vectors of observations on the dependent and explanatory variables respectively, and $T$ is the sample size, $E$ is a $T\\times1$ vector stacking the error terms, and $\\beta$ is a scalar parameter. Conditionally on $X$, $E$ is normally distributed with the mean set to a $T$-column vector of zeros, denoted by $\\mathbf{0}_T$, and covariance matrix equal to $\\sigma^2I_T$, where $\\sigma^2$ is individual error term variance, and $I_T$ is an identity matrix of order $T$.The predictive density of data conditional of the explanatory variables and the parameters of the model that is implied by the model equations above is specified as:\n\\begin{equation} \nY|X,\\beta,\\sigma^2 \\sim\\mathcal{N}\\left(\\beta X, \\sigma^2I_T\\right).\n\\end{equation} \n\n\n## Likelihood function\n\nLet vector $\\theta=\\left(\\beta,\\sigma^2\\right)'$ collect the parameters of the model. Then, the likelihood function takes the following form:\n\\begin{equation}\nL\\left(\\theta|Y,X\\right) = (2\\pi)^{-\\frac{T}{2}} \\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}(Y-\\beta X)'(Y-\\beta X) \\right\\}\n\\end{equation}\nWe show that the likelihood function has the form of a normal inverse gamma 2 distribution for the parameters of the model $\\beta$ and $\\sigma^2$.\n\\begin{align}\nL\\left(\\theta|Y,X\\right) &\\propto \\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}(Y-\\beta X)'(Y-\\beta X) \\right\\}\\\\[1ex]\n&= \\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}(Y-\\hat\\beta X +\\hat\\beta X - \\beta X)'(Y-\\hat\\beta X +\\hat\\beta X-\\beta X) \\right\\}\\label{eq:likeli1}\\\\[1ex]\n&= \\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}\\left[ (\\beta - \\hat\\beta)'X'X (\\beta - \\hat\\beta) + (Y-\\hat\\beta X)'(Y-\\hat\\beta X)\\right] \\right\\}\\label{eq:likeli2}\\\\[1ex]\n&= \\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2} (\\beta - \\hat\\beta)'X'X (\\beta - \\hat\\beta) \\right\\} \\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}  (Y-\\hat\\beta X)'(Y-\\hat\\beta X) \\right\\}\\label{eq:likeli3}\n\\end{align}\nwhere in the second line above we add and subtract $\\hat\\beta X$, where $\\hat\\beta = (X'X)^{-1}X'Y$ is the Maximum Likelihood Estimator of $\\beta$, and then, simply regroup and cancel out appropriate elements to go from line \\eqref{eq:likeli1} to \\eqref{eq:likeli2}.\n\nThe outcome of the derivation is the kernel of the normal inverse gamma 2 distribution. To see this, reorder the terms in equation \\eqref{eq:likeli3} the following way:\n\\begin{equation}\n\\underbrace{\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2} (\\beta - \\hat\\beta)'X'X (\\beta - \\hat\\beta) \\right\\}}_{\\text{normal part for }\\beta|\\sigma^2}\n\\underbrace{\\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}  (Y-\\hat\\beta X)'(Y-\\hat\\beta X) \\right\\}}_{\\text{inverse gamma 2 part for }\\sigma^2},\n\\end{equation}\nand note that two parts can be easily identified. One is the kernel of the normal distribution of $\\beta$ given $\\sigma^2$, and the other is the kernel of the inverse gamma 2 distribution for $\\sigma^2$.\n\nWe can, therefore, write that the likelihood function implies the following distribution for the parameters of the model:\n\\begin{equation}\nL\\left(\\theta|Y,X\\right) = \\mathcal{NIG}2_{(N=1)}\\left(\\hat\\beta, (X'X)^{-1}, (Y-\\hat\\beta X)'(Y-\\hat\\beta X),  T-3 \\right)\n\\end{equation}\n\n\n## A natural-conjugate prior distribution\n\nA natural-conjugate prior distribution is of the same form as the distribution of the parameters implied by the likelihood function. More importantly, it implies the joint posterior distribution in a form of the same parametric distribution, which is shown in Section \\ref{sec:posterior}. The naturally-conjugate prior distribution for parameters $\\beta$ and $\\sigma^2$ is of normal inverse gamma 2 form. More specifically, it is specified by a normal conditional distribution of $\\beta$ given $\\sigma^2$ and a marginal inverse gamma 2 prior distribution for $\\sigma^2$:\n\\begin{align}\np\\left(\\beta, \\sigma^2\\right) &= p\\left(\\beta|\\sigma^2\\right)p\\left(\\sigma^2\\right),\n\\end{align}\nwhere the individual distributions are as follows:\n\\begin{align}\np\\left(\\beta|\\sigma^2\\right)&=\\mathcal{N}\\left( \\underline{\\beta}, \\sigma^2\\underline{\\sigma}_{\\beta}^2 \\right)\\\\\np\\left(\\sigma^2\\right)&=\\mathcal{IG}2(\\underline{s},\\underline{\\nu}).\n\\end{align}\nTherefore, we write down its kernel as:\n\\begin{equation}\\label{eq:prior}\n\\mathcal{NIG}2_{(N=1)}\\left(\\underline{\\beta}, \\underline{\\sigma}_{\\beta}^2, \\underline{s},\\underline{\\nu} \\right) \\propto \\left(\\sigma^2\\right)^{-\\frac{\\underline{\\nu}+3}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}\\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) \\right\\}\n\\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2} \\right\\}\n\\end{equation} \n\n\n\n## Joint posterior distribution\n\nIn this section, we derive an analytical solution to the joint posterior distribution of a simple regression model with natural-conjugate prior distribution in a form of the normal inverse gamma 2 distribution.\n\nThe posterior distribution is proportional to the product of the likelihood function and the prior distribution:\n\\begin{align}\np\\left( \\beta,\\sigma^2|Y,X \\right) &\\propto L\\left( Y|X,\\beta,\\sigma^2 \\right)p\\left( \\beta,\\sigma^2 \\right)\\\\\n&= L\\left( Y|X,\\beta,\\sigma^2 \\right)p\\left( \\beta|\\sigma^2 \\right)p\\left( \\sigma^2 \\right)\n\\end{align}\nWe plug in the corresponding expressions for the likelihood function from equation \\eqref{eq:likeli3} and the prior distribution from equation \\eqref{eq:prior} to obtain:\n\\begin{align}\np\\left( \\beta,\\sigma^2|Y,X \\right) &\\propto \n\\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2} (\\beta - \\hat\\beta)'X'X (\\beta - \\hat\\beta) \\right\\} \\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}  (Y-\\hat\\beta X)'(Y-\\hat\\beta X) \\right\\}\\\\\n&\\quad\\times \\left(\\sigma^2\\right)^{-\\frac{\\underline{\\nu}+3}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}\\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) \\right\\}\n\\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2} \\right\\}\n\\end{align}\nwhich, after collecting corresponding powers of $\\sigma^2$ and arguments of the exponential function, can be written as:\n\\begin{equation}\\label{eq:post-kernel}\np\\left( \\beta,\\sigma^2|Y,X \\right) \\propto \n\\left( \\sigma^2 \\right)^{-\\frac{\\underline{\\nu}+T+3}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2} \n\\left[ \\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) + (\\beta - \\hat\\beta)'X'X (\\beta - \\hat\\beta) + \\underline{s} + (Y-\\hat\\beta X)'(Y-\\hat\\beta X) \\right] \\right\\}\n\\end{equation}\n\nTo derive the parameters of the posterior distribution we will now focus on expression in square parentheses in the exponential function. The idea is to derive the following quadratic form, $\\overline{\\sigma}_{\\beta}^{-2}(\\beta-\\overline{\\beta})'(\\beta-\\overline{\\beta})$ and separate it from the rest of the elements. The derivation of the quadratic form requires completing the squares. The quadratic form will be used to construct the normal distribution part of the posterior distribution, whereas the remaining elements will be used to construct its inverse gamma 2 part.\n\nThe first step is to multiply all of the elements:\n\\begin{multline}\n\\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) + (\\beta - \\hat\\beta)'X'X (\\beta - \\hat\\beta) + \\underline{s} + (Y-\\hat\\beta X)'(Y-\\hat\\beta X)\\\\\n= \\beta^2\\underline{\\sigma}_{\\beta}^{-2} - \\beta 2 \\underline{\\beta} \\underline{\\sigma}_{\\beta}^{-2} + \\underline{\\beta}^2 \\underline{\\sigma}_{\\beta}^{-2}+ \\beta^2X'X - \\beta 2 \\hat\\beta X'X + \\hat\\beta^2 X'X + \\underline{s} + Y'Y -2\\hat\\beta X'Y + \\hat\\beta^2 X'X\n\\end{multline}\nThe second step is to collect the elements containing $\\beta$ and $\\beta^2$ to obtain:\n\\begin{equation}\n= \\beta^2\\left( \\underline{\\sigma}_{\\beta}^{-2} + X'X\\right) - \\beta 2 \\left( \\underline{\\beta} \\underline{\\sigma}_{\\beta}^{-2} + \\hat\\beta X'X \\right) + \\underline{\\beta}^2 \\underline{\\sigma}_{\\beta}^{-2}  + \\underline{s} + Y'Y -2\\hat\\beta X'Y + 2\\hat\\beta^2 X'X\n\\end{equation}\nIt is easy to show that the last two elements in the formula above, $-2\\hat\\beta X'Y + 2\\hat\\beta^2 X'X$, cancel out.\n\nLet $\\overline{\\sigma}_{\\beta}^{-2} = \\left( \\underline{\\sigma}_{\\beta}^{-2} + X'X\\right)$. Now, plug in $\\overline{\\sigma}_{\\beta}^{-2}$ in the expression above and also multiply and divide its second element by $\\overline{\\sigma}_{\\beta}^{2}$:\n\\begin{equation}\n= \\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta 2 \\left( \\underline{\\beta} \\underline{\\sigma}_{\\beta}^{-2} + \\hat\\beta X'X \\right)\\overline{\\sigma}_{\\beta}^{2}\\overline{\\sigma}_{\\beta}^{-2} + \\underline{\\beta}^2 \\underline{\\sigma}_{\\beta}^{-2}  + \\underline{s} + Y'Y \n\\end{equation}\nLet $\\overline{\\beta} = \\left( \\underline{\\beta} \\underline{\\sigma}_{\\beta}^{-2} + \\hat\\beta X'X \\right)\\overline{\\sigma}_{\\beta}^{2}$. In the next step, plug in $\\overline{\\beta}$ in the expression above and also add and subtract term $\\overline{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2}$ to obtain:\n\\begin{equation}\n= \\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta 2 \\overline{\\beta}\\overline{\\sigma}_{\\beta}^{-2} + \\overline{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2} - \\overline{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2} + \\underline{\\beta}^2 \\underline{\\sigma}_{\\beta}^{-2}  + \\underline{s} + Y'Y \n\\end{equation}\nNote that the first three terms in above can be written as $\\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta 2 \\overline{\\beta}\\overline{\\sigma}_{\\beta}^{-2} + \\overline{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2} = \\overline{\\sigma}_{\\beta}^{-2}\\left(\\beta-\\overline{\\beta}\\right)'\\left(\\beta-\\overline{\\beta}\\right)$. \n\nTherefore, we conclude our derivation and obtain:\n\\begin{equation}\n= \\overline{\\sigma}_{\\beta}^{-2}\\left(\\beta-\\overline{\\beta}\\right)'\\left(\\beta-\\overline{\\beta}\\right) + \\underline{s} + \\underline{\\beta}^2 \\underline{\\sigma}_{\\beta}^{-2} - \\overline{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2}  + Y'Y \n\\end{equation}\nAfter plugging in the expression above back to the density function of the posterior distribution in equation \\eqref{eq:post-kernel} we obtain:\n\\begin{align}\np\\left( \\beta,\\sigma^2|Y,X \\right) &\\propto \n\\left( \\sigma^2 \\right)^{-\\frac{\\underline{\\nu}+T+3}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2} \n\\frac{1}{\\overline{\\sigma}_{\\beta}^{2}}\\left(\\beta-\\overline{\\beta}\\right)'\\left(\\beta-\\overline{\\beta}\\right) \\right\\} \\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2} \\left( \\underline{s} + \\underline{\\beta}^2 \\underline{\\sigma}_{\\beta}^{-2} - \\overline{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2}  + Y'Y  \\right)  \\right\\}\\\\\n&=  \\left(\\sigma^2\\right)^{-\\frac{\\overline{\\nu}+3}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}\\frac{1}{\\overline{\\sigma}_{\\beta}^2}\\left(\\beta-\\overline{\\beta}\\right)'\\left(\\beta-\\overline{\\beta}\\right) \\right\\}\n\\exp\\left\\{ -\\frac{1}{2}\\frac{\\overline{s}}{\\sigma^2} \\right\\}\n\\end{align}\nwhich fully defines the joint posterior distribution as the normal inverse gamma 2 distribution with parameters $\\overline{\\beta}$, $\\overline{\\sigma}_{\\beta}^2$, $\\overline{s}$, and $\\overline{\\nu}$ given by:\n\\begin{align*} \np\\left( \\beta,\\sigma^2|Y,X \\right) &= \\mathcal{NIG}2_{(N=1)}\\left(\\overline{\\beta}, \\overline{\\sigma}_{\\beta}^2, \\overline{s},\\overline{\\nu} \\right)\\\\[1ex]\n\\overline{\\sigma}_{\\beta}^2 &= \\left( \\underline{\\sigma}_{\\beta}^{-2} + X'X \\right)^{-1} \\\\\n\\overline{\\beta} &= \\overline{\\sigma}_{\\beta}^2\\left( \\underline{\\sigma}_{\\beta}^{-2}\\underline{\\beta} + X'Y \\right) \\\\ \n\\overline{s} &= \\underline{s} + \\underline{\\sigma}_{\\beta}^{-2}\\underline{\\beta}^2 - \\overline{\\sigma}_{\\beta}^{-2}\\overline{\\beta}^2 + Y'Y \\\\\n\\overline{\\nu} &= \\underline{\\nu} + T\n\\end{align*} \nThe distribution above fully characterizes our knowledge about the parameters of the model after updating the prior distribution with the information from the data.\n\n\n\n## A conditionally-conjugate prior distribution\n\n\nIn the subsequent two sections, we consider an alternative way of setting the prior distribution that leads to an alternative estimation procedure. \n\nA conditionally-conjugate prior distribution implies the corresponding full conditional posterior distribution in the same functional form. To assure this property, we assume that $\\beta$ and $\\sigma^2$ are \\emph{a priori} independent and that the parameters follow the following independent normal inverse gamma 2 distribution.\n\\begin{align}\np\\left(\\beta, \\sigma^2\\right) &= p\\left(\\beta\\right)p\\left(\\sigma^2\\right)\\\\[1ex]\np\\left(\\beta\\right)&=\\mathcal{N}\\left( \\underline{\\beta}, \\underline{\\sigma}_{\\beta}^2 \\right)\\\\\np\\left(\\sigma^2\\right)&=\\mathcal{IG}2(\\underline{s},\\underline{\\nu})\n\\end{align}\nTherefore, we write down its kernel as:\n\\begin{equation} \np\\left(\\beta, \\sigma^2\\right) \\propto \n\\underbrace{\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) \\right\\}}_{\\text{normal part for }\\beta} \n\\times \n\\underbrace{\\left(\\sigma^2\\right)^{-\\frac{\\underline{\\nu}+2}{2}}\n\\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2} \\right\\}}_{\\text{inverse gamma part for }\\sigma^2}\n\\end{equation} \n\n\n\n## Full conditional posterior distributions\n\nThe prior independence of $\\beta$ and $\\sigma^2$ implies an intractable form of the joint posterior distribution. However, the independent normal inverse gamma 2 distribution belongs to a class of conditionally-conjugate prior distributions and implies that the full conditional posterior distributions $p\\left(\\beta|Y,X,\\sigma^2\\right)$ and $p\\left(\\sigma^2|Y,X,\\beta\\right)$ are in a form of known distributions. We derive these distributions below and specify the Gibbs sampler using them.\n\nIn the first step, we derive the full conditional posterior distribution for $beta$ given, $Y$, $X$, and $\\sigma^2$, that is denoted by $p\\left(\\beta|Y,X,\\sigma^2\\right)$. Conditioning on $\\sigma^2$ implies that, for the sake of deriving the full conditional posterior distribution of $\\beta$, we treat it as non-random and, therefore, any elements that contain $\\sigma^2$ that do not contain $\\beta$ can be omitted when working with the kernel of $p\\left(\\beta|Y,X,\\sigma^2\\right)$.\n\\begin{align} \np\\left(\\beta|Y,X,\\sigma^2\\right) &\\propto L\\left(\\beta,\\sigma^2|Y,X\\right)p\\left(\\beta\\right)\\\\\n&= \\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}(Y-\\beta X)'(Y-\\beta X) \\right\\} \\times\n\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) \\right\\}\\\\\n&= \\exp\\left\\{ -\\frac{1}{2}\\left[\\frac{1}{\\sigma^2}(Y-\\beta X)'(Y-\\beta X) + \\frac{1}{\\underline{\\sigma}_{\\beta}^2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) \\right]\\right\\}\\label{eq:betafullcond}\n\\end{align} \n\nFocus on the expression in square parentheses and transform it to a quadratic form $\\frac{1}{\\overline{\\sigma}_{\\beta}^2}(\\beta-\\overline{\\beta})'(\\beta-\\overline{\\beta})$, where $\\overline{\\beta}$ and $\\overline{\\sigma}_{\\beta}^2$ denote the parameters of the full conditional posterior distribution for $\\beta$.\n\nIn this step of the derivation, we multiply all of the elements and then rearrange them collecting elements containing $\\beta^2$ and $\\beta$ respectively dropping all others elements from the kernel of the distribution:\n\\begin{align} \n\\sigma^{-2}(Y-\\beta X)'(Y-\\beta X) + \\underline{\\sigma}_{\\beta}^{-2}(\\beta-\\underline{\\beta})'(\\beta-\\underline{\\beta}) \n&= \\sigma^{-2}Y'Y -\\beta2X'Y\\sigma^{-2} + \\beta^2X'X\\sigma^{-2} + \\beta^2\\underline{\\sigma}_{\\beta}^{-2} - \\beta2\\underline{\\beta}\\underline{\\sigma}_{\\beta}^{-2}+ \\underline{\\beta}^{2}\\underline{\\sigma}_{\\beta}^{-2}\\\\\n&= \\beta^2\\left(X'X\\sigma^{-2} + \\underline{\\sigma}_{\\beta}^{-2}\\right) - \\beta2\\left(X'Y\\sigma^{-2} + \\underline{\\beta}\\underline{\\sigma}_{\\beta}^{-2}\\right) + \\dots\n\\end{align} \nLet $\\overline{\\sigma}_{\\beta}^2=\\left(X'X\\sigma^{-2} + \\underline{\\sigma}_{\\beta}^{-2}\\right)^{-1}$, and multiply and divide the second element on the right-hand side of the second line above by $\\overline{\\sigma}_{\\beta}^2$ to obtain:\n\\begin{align} \n\\propto \\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta2\\left(X'Y\\sigma^{-2} + \\underline{\\beta}\\underline{\\sigma}_{\\beta}^{-2}\\right)\\overline{\\sigma}_{\\beta}^2 \\overline{\\sigma}_{\\beta}^{-2} \n&= \\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta2 \\overline{\\beta} \\overline{\\sigma}_{\\beta}^{-2}\\\\\n&= \\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta2 \\overline{\\beta} \\overline{\\sigma}_{\\beta}^{-2} + \\overline{\\beta}^2\\overline{\\sigma}_{\\beta}^{-2} + \\dots\n\\end{align} \nNote that on the right-hand side of the first line above we plugged in $\\overline{\\beta} = \\left(X'Y\\sigma^{-2} + \\underline{\\beta}\\underline{\\sigma}_{\\beta}^{-2}\\right)\\overline{\\sigma}_{\\beta}^2$, and in the second line we added and subtracted from the expression element $\\overline{\\beta}^2\\overline{\\sigma}_{\\beta}^{-2}$. Then, we dropped the element with the negative sign as we do not need it in the kernel of the distribution for $\\beta$. Finally, we obtain the required quadratic form:\n\\begin{equation} \n\\beta^2\\overline{\\sigma}_{\\beta}^{-2} - \\beta2 \\overline{\\beta} \\overline{\\sigma}_{\\beta}^{-2} + \\overline{\\beta}^2\\overline{\\sigma}_{\\beta}^{-2} = \\overline{\\sigma}_{\\beta}^{-2}(\\beta-\\overline{\\beta})'(\\beta-\\overline{\\beta})\n\\end{equation} \nwhich we now plug in back again to the expression in equation \\eqref{eq:betafullcond}:\n\\begin{equation} \np\\left(\\beta|Y,X,\\sigma^2\\right) \\propto  \\exp\\left\\{ -\\frac{1}{2}\\overline{\\sigma}_{\\beta}^{-2}(\\beta-\\overline{\\beta})'(\\beta-\\overline{\\beta})\\right\\}\n\\end{equation} \nin which we recognize the kernel of a normal distribution.\n\nTherefore, we the full conditional posterior distribution of $\\beta$ is a normal distribution:\n\\begin{align} \np\\left(\\beta|Y,X,\\sigma^2\\right) &=\\mathcal{N}\\left(\\overline{\\beta}, \\overline{\\sigma}_{\\beta}^2\\right)\\\\\n\\overline{\\sigma}_{\\beta}^2 &= \\left( \\underline{\\sigma}_{\\beta}^{-2}+ \\sigma^{-2}X'X \\right)^{-1}\\\\\n\\overline{\\beta} &= \\overline{\\sigma}_{\\beta}^2\\left( \\underline{\\sigma}_{\\beta}^{-2}\\underline{\\beta} + \\sigma^{-2}X'Y \\right)\n\\end{align} \n\n\nIn the second step of the derivations, we proceed similarly to derive the full conditional posterior distribution of $\\sigma^2$ given $Y$, $X$, and $\\beta$, that is denoted by $p\\left(\\sigma^2|Y,X,\\beta\\right)$. Conditioning on $\\beta$ implies that, for the sake of deriving the full conditional posterior distribution of $\\sigma^2$, we treat it as non-random and, therefore, any elements that contain $\\beta$ and do not contain $\\sigma^2$ can be omitted when working with the kernel of $p\\left(\\sigma^2|Y,X,\\beta\\right)$.\n\\begin{align} \np\\left(\\sigma^2|Y,X,\\beta\\right) &\\propto L\\left(\\beta,\\sigma^2|Y,X\\right)p\\left(\\sigma^2\\right)\\\\\n&= \\left( \\sigma^2 \\right)^{-\\frac{T}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}(Y-\\beta X)'(Y-\\beta X) \\right\\} \\times\n\\left(\\sigma^2\\right)^{-\\frac{\\underline{\\nu}+2}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2} \\right\\}\\\\\n&= \\left( \\sigma^2 \\right)^{-\\frac{T+\\underline{\\nu}+2}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{1}{\\sigma^2}\\left[(Y-\\beta X)'(Y-\\beta X) + \\underline{s} \\right]\\right\\}\\label{eq:fcsi1}\\\\\n&= \\left( \\sigma^2 \\right)^{-\\frac{\\overline{\\nu}+2}{2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{\\overline{s}}{\\sigma^2}\\right\\}\\label{eq:fcsi2}\n\\end{align} \nIn line \\eqref{eq:fcsi1}, we rearranged the elements, and in line \\eqref{eq:fcsi2} expressions for $\\overline{\\nu} = T+\\underline{\\nu}$ and $\\overline{s} = (Y-\\beta X)'(Y-\\beta X) + \\underline{s}$ were plugged in. The final line specifies the kernel of the following inverse gamma 2 distribution:\n\\begin{align} \np\\left(\\sigma^2|Y,X,\\beta\\right) &= \\mathcal{IG}2\\left( \\overline{s}, \\overline{\\nu} \\right)\\\\[1ex]\n\\overline{s} &= \\underline{s} + (Y-\\beta X)'(Y-\\beta X) \\\\\n\\overline{\\nu} &= \\underline{\\nu} +T \n\\end{align}\n\n## Gibbs sampler\n\nThe Gibbs sampler for the simple linear regression is given by the following algorithm:\n\\begin{description}\n\\item[Initialize] $\\sigma^2$ at some positive value $\\sigma^{2(0)}$\n\\item[At each iteration] $s$:\n\\begin{description}\n\\item[1. Draw] $\\beta^{(s)}\\sim p\\left(\\beta|Y,X,\\sigma^{2(s-1)}\\right) = \\mathcal{N}\\left(\\overline{\\beta}, \\overline{\\sigma}_{\\beta}^2\\right)$\n\\item[2. Draw] $\\sigma^{2(s)}\\sim p\\left(\\sigma^2|Y,\\beta^{(s)}\\right)=\\mathcal{IG}2\\left( \\overline{s}, \\overline{\\nu} \\right)$\n\\end{description}\n\\item[Repeat] steps 1. and 2. $S_1 + S_2$ times\n\\item[Discard] the first $S_1$ draws that allowed the algorithm to converge to the stationary posterior distribution\n\\item[Output] is a sample of draws from the joint posterior distribution $\\left\\{ \\beta^{(s)}, \\sigma^{2(s)} \\right\\}_{s=S_1+1}^{S_2}$\n\\end{description}\n\nThe implementation of this Gibbs sampler in R requires a feasible random number generators from the normal and inverse gamma 2 distributions. To sample random numbers from the normal distribution, $\\mathcal{N}_1\\left( \\mu, \\sigma^2 \\right)$, use function \\texttt{rnorm()} available from the \\texttt{stats} package that is uploaded to the memory by default upon opening R. To sample random numbers from a multivariate normal distribution, $\\mathcal{N}_N\\left( \\mu, \\Sigma \\right)$, use function \\texttt{rmvnorm()} from package \\texttt{mvtnorm}. Sampling random numbers from the inverse gamma 2 distribution, $\\mathcal{IG}2\\left( s, \\nu \\right)$, requires a two-step procedure. In the first step, draw a random number from $\\bar{s}\\sim\\chi^2(\\nu)$ using function \\texttt{rchisq()}. In the second step, return $s/\\bar{s}$ as a draw from $\\mathcal{IG}2\\left( s, \\nu \\right)$.\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"regression.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.38","bibliography":["references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"regression.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{}}}}}